LINEAR REGRESSION
	Terminologies:
		m = no. of training data sets
		x = input variable
		y = output variable
		h = hypothesis(a function) which takes input and predicts output
		https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/H6qTdZmYEeaagxL7xdFKxA_2f0f671110e8f7446bb2b5b2f75a8874_Screenshot-2016-10-23-20.14.58.png?expiry=1522627200000&hmac=GnYqYkeFod1xRbu4BplrAVY5XxTTxyjA8N14nTFLmGs
		A pair (x^(i),y^(i)) is called a training example.
		Here y varies linearly with x. Also called univariable regression.
Cost Function
	 This takes an average difference (actually a fancier version of an average) of all the 
results of the hypothesis with inputs from x's and the actual output y's.
	This function is otherwise called the "Squared error function", or "Mean squared error". 
	The mean is halved 1/2 as a convenience for the computation of the gradient descent, as 
the derivative term of the square function will cancel out the 1/2 term.