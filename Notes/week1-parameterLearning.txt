Gradient descent
	To minimize cost function
	A general algorithm used to minimize functions.
	Terminology:
		a := a + 1 is equivalent to a += 1. a = a + 1 is an assertion with is always false.
	Look around and take the steps where the derivative of J(cost function) decreases, until it 
becomes zero, i.e., arrive at minima.
	"Batch" gradient descent. Each step uses all training examples. Uses all "m" datasets to 
calculate J
	Gradient descent can be susceptible to local minima in general, but Linear regression 
has only one global, and no other local optima; thus gradient descent always converges 
(assuming the learning rate Î± is not too large) to the global minimum.