# Machine Learning
### By Stanford University on Coursera
##### Conducted by Andrew Ng

This course provides a broad introduction to machine learning, datamining, and statistical pattern recognition. Topics include: (i) Supervised learning (parametric/non-parametric algorithms, support vector machines, kernels, neural networks). (ii) Unsupervised learning (clustering, dimensionality reduction, recommender systems, deep learning). (iii) Best practices in machine learning (bias/variance theory; innovation process in machine learning and AI). The course will also draw from numerous case studies and applications, so that you'll also learn how to apply learning algorithms to building smart robots (perception, control), text understanding (web search, anti-spam), computer vision, medical informatics, audio, database mining, and other areas.

#### Week 1
  -  Introduction about supervised learning and unsupervised learning
  -  Understood about Regression and Classification problems under supervised learning
  -  Clustering and Cocktail-party algorithm under unsupervised learning. 
#### Week 2
  - Linear Regression in one and multiple variables
  - Cost function and Gradient descent
  - Feature Normalizations
  - Normal equation.
#### Week 3
  -  Binary classification problem
  -  Logistic regression
  -  Sigmoidal function as hypothesis
  -  Advanced optimization algorithm
  -  fminunc() of Octave
  -  Regularization to avoid over-fitting
  -  Introduction to Multiclass classification problem. 
#### Week 4
  - Introduction to Neural Networks
  - Layers of a neural network
  - Calculation of activation values of each layer
  - Forward Propagation
  - Representation of logical operators like AND, OR, XNOR, etc with simple neural network
  - Training multiclass classification by One-VS-All method. 
#### Week 5
  - Cost function of a neural network
  - Backpropagation algorithm to find gradient
  - Using gradient checking
  - Random initialization to break the symmetry
#### Week 6
  - Machine Learning Diagnostic
  - Evaluating the hypothesis
  - Model selection
  - Recognizing High bias or high variance from learning curves
  - Debugging depending whether there's high bias or high variance
  - Skewed classes
  - Precision/Recall and F1 score to evaluate learning algorithm
#### Week 7
  - Support Vector Machine (Large Margin Classifier)
  - Kernels
  - Gaussian Kernel
  - Selection of SVM Parameters
#### Week 8
  - Unsupervised learning
  - Clustering - K Means Algorithm
  - Dimensionality Reduction-PCA
#### Week 9
  - Anomaly detection
  - Gaussian Distribution
  - Multi-variate Gaussian Distribution
  - Recommender system:
    - Content-based
    - Collaborative Filtering
#### Week 10
  - Large Scale Machine Learning
    - Stochastic Gradient Descent
    - Mini-batch Gradent Descent
    - Online Learning
#### Week 11
  - Photo OCR Problem
    - Sliding window detection
    - Artificial data synthesis
  - Machine learning pipeline
  - Ceiling Analysis: What Part of the Pipeline to Work on Next

> What would it take to get 10x more data as we have now?
